{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis mit TF-IDF-Classifier\n",
    "\n",
    "_Anmerkung: Dieser Artikel ist eine Ergänzung zum its-people-Webinar \"Do you speak NLP - Ein Streifzug durch modernes Natural Language Processing mit Python\", gehalten im Oktober 2021 von Markus Zeeb und Uwe Blenz (und tatkräftiger Unterstützung durch Andreas Hoffmann und Thomas und Birgit Krämer)._\n",
    "\n",
    "In diesem Artikel wollen wir uns mit dem Thema \"Sentiment Analysis mit TF-IDF Classifiers\" beschäftigen. Dazu werden wir uns im ersten Teil allgemein mit TF-IDF befassen, die Grundidee dahinter beleuchten und Schritt für Schritt einen eigenen, kleinen TF-IDF-\"Classifier\" implementieren (Python). Im zweiten Teil werden wir unsere Erkenntnisse aus dem ersten Teil nutzen, um einen weiteren TF-IDF-Classifier zu bauen und darauf trainieren, Produktreviews von Amazon.com in die Kategorien \"positiv\" und \"negativ\" einzuordnen.\n",
    "\n",
    "## Teil 1 - Kategorisierung von Texten\n",
    "\n",
    "tf-idf steht für \"term frequency - inverse document frequency\" und ist ein Verfahren, mit dem Texte (bzw. \"Dokumente\") in verschiedene Kategorien eingeordnet werden können, bspw. Fachartikel nach Disziplin, Bücher nach Genre usw.\n",
    "\n",
    "Allgemein gehen wir bei tf-idf davon aus, dass wir eine sehr große Sammlung von Dokumenten (linguist. \"Corpus\" genannt) haben, die wir klassifizieren wollen; da es uns im Moment aber rein darum geht zu verstehen, wie tf-idf überhaupt funktioniert, nehmen wir für diesen Zweck folgenden, sehr übersichtlichen Corpus an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"foo bla bla foo\",\n",
    "    \"bla bla bar bla\",\n",
    "    \"baz bla bla foo bla bla baz\",\n",
    "    \"bla bla bla\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsere Aufgabe soll nun darin bestehen, die Dokumente (wenn möglich), den Kategorien \"foo\", \"bar\" und \"baz\" zuzuordnen. Da unser Beispielskorpus lediglich aus 4 Dokumenten besteht, erkennen wir natürlich sofort, welche Dokumente in welche Kategorien gehören: Wir sehen bspw. sofort, dass der Term \"foo\" zwei Mal in Dokument 1 enthalten ist und dieses folglich zur Kategorie \"foo\" gehört. Interessanter wird es in Dokument 3, das zwei Mal \"baz\" und ein Mal \"foo\" enthält. Da wir die Zuordnung in mehrere Kategorien erstmal nicht erlauben, nehmen wir für Dokument 3 Kategorie \"baz\" an, da \"baz\" häufiger auftritt als \"foo\".\n",
    "\n",
    "Wir halten also fest:\n",
    "\n",
    "> Je häufiger ein Begriff (eng. \"term\") in einem Dokument vorkommt, desto relevanter (bzw. \"informativer\") ist er für dieses Dokument.\n",
    "\n",
    "Diese Beobachtung stellt den Kern des tf-idf-Verfahrens dar und deshalb soll das erste Ziel auf dem Weg zu unserer kleinen tf-idf-Implementierung genau das sein: eine Liste pro Dokument, die angibt, wie oft (häufig) ein Begriff in diesem Dokument vorkommt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Set\n",
    "\n",
    "def build_vocabulary(docs: List[str]) -> Set[str]:\n",
    "    vocabulary = set()\n",
    "    for doc in docs:\n",
    "        vocabulary = vocabulary.union(doc.split())\n",
    "    return vocabulary\n",
    "\n",
    "def prepare_documents(docs: List[str]) -> pd.DataFrame:\n",
    "    vocabulary = build_vocabulary(docs)\n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        vec = dict.fromkeys(vocabulary, 0)\n",
    "        for word in doc.split():\n",
    "            vec[word] += 1\n",
    "        results.append(vec)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foo</th>\n",
       "      <th>bla</th>\n",
       "      <th>baz</th>\n",
       "      <th>bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   foo  bla  baz  bar\n",
       "0    2    2    0    0\n",
       "1    0    3    0    1\n",
       "2    1    4    2    0\n",
       "3    0    3    0    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = prepare_documents(documents)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir sehen können, sind die Dokumente in unserem Corpus jetzt keine Texte mehr, sondern Listen, wie häufig ein Begriff in einem Dokument vorkommt (inkl. \"0 Mal\", wenn ein Begriff also nicht im Dokument auftaucht).\n",
    "\n",
    "Aus technischer Perspektive sind hier zwei Sachen passiert:\n",
    "\n",
    "1. Die Dokumente wurden vektorisiert, d.h. wir haben textuelle Daten in numerische Vektoren mit einheitlicher Länge überführt.\n",
    "2. Jeder Vektor ist gleichzeitig auch ein \"bag of words\", in dem für jeden Term in unserem Corpus angegeben wird, wie oft er in einem bestimmten Dokumten auftaucht.\n",
    "\n",
    "Theoretisch haben wir damit unser erstes Ziel auch schon erreicht: Wir haben nun für jedes Dokument eine Liste mit \"term frequencies\", und tatsächlich arbeiten einige Implementierungen von tf-idf mit dieser Definition von \"term frequency\". \n",
    "\n",
    "Wir wollen uns damit aber noch nicht zufrieden geben und definieren \"term frequency\" als \"relative Häufigkeit\" von \"absolute Häufigkeit eines Terms\" zu \"Anzahl aller Terme eines Dokuments\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foo</th>\n",
       "      <th>bla</th>\n",
       "      <th>baz</th>\n",
       "      <th>bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        foo       bla       baz   bar\n",
       "0  0.500000  0.500000  0.000000  0.00\n",
       "1  0.000000  0.750000  0.000000  0.25\n",
       "2  0.142857  0.571429  0.285714  0.00\n",
       "3  0.000000  1.000000  0.000000  0.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def term_frequency(docs: pd.DataFrame):\n",
    "    res = []\n",
    "    for row in docs.values:\n",
    "        res.append(np.divide(row, row.sum()))\n",
    "    return pd.DataFrame(res, columns=docs.columns)\n",
    "\n",
    "term_frequency(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So weit, so gut. Meinen aufmerksamen Lesern wird aber sicher nicht entgangen sein, dass ich den \"Elefanten im Raum\" bisher geflissentlich ignoriert habe, nämlich: \"bla\". \n",
    "\n",
    "Der Term \"bla\" taucht mit Abstand am häufigsten auf und wenn wir uns stumpf an unsere Definition von \"Relevanz\" weiter oben hielten, gehörten alle unsere Dokumente der Kategorie \"bla\" an. \n",
    "\n",
    "Das möchten wir natürlich unbedingt vermeiden und deshalb den \"Relevanz-Score\" von Wörtern, die allgemein häufig (d.h. in vielen und unterschiedlichen Dokumenten) in einem Corpus vorkommen, weniger stark gewichtet werden als solche, die allgemein eher selten sind.\n",
    "\n",
    "Um das zu erreichen, zählen wir erst einmal für jeden Begriff, in wievielen Dokumenten er vorkommt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo    2\n",
       "bla    4\n",
       "baz    1\n",
       "bar    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def document_frequency(docs: pd.DataFrame):\n",
    "    return docs.applymap(lambda n: 1 if n > 0 else 0).sum()\n",
    "\n",
    "document_frequency(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das führt uns wieder zu einer Häufigkeit, dieses Mal nämlich die sog. \"Dokumentenhäufigkeit\" (eng. \"document frequency\"). \n",
    "\n",
    "In dieser Form ist document frequency eine absolute Häufigkeit. Würden wir hierzu die relativen Häufigkeiten berechnen (d.h. durch die Gesamtzahl an Dokumenten in unserem Corpus teilen), ergäben sich die folgenden Werte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo    0.50\n",
       "bla    1.00\n",
       "baz    0.25\n",
       "bar    0.25\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_frequency(documents).div(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Werte können wir so aber nicht als Gewichte verwenden, da sie immer noch nur solche Begriffe favourisieren, die über den ganzen Corpus hinweg häufig vorkommen. Erreichen wollen wir aber genau das Gegenteil! \n",
    "\n",
    "Erinnern wir uns nochmal kurz an den Namen des Verfahrens: \"term frequency - **inverse** document frequency\" -- \"Aha!\", wir nun der Lateiner rufen, \"_invers_, von lat. _inversus_, also 'umgedreht' oder 'auf den Kopf gestellt'! Könnte das die Lösung sein?\" Und tatsächlich, wenn wir die Relation einfach umdrehen und statt document frequency\n",
    "\n",
    "$$\\text{df}(t) = \\frac{|\\{ doc \\in corpus : t \\in doc \\}|}{N}$$\n",
    "\n",
    "einfach die \"inverse document frequency\"\n",
    "\n",
    "$$\\text{idf}(t) = \\frac{N}{|\\{ doc \\in corpus : t \\in doc \\}|}$$\n",
    "\n",
    "verwenden (mit $|\\{ doc \\in corpus : t \\in doc \\}|$ = \"Anz. Dokumente, die _t_ enthalten\" und $N$ = \"Anz. Dokumente in Corpus\"), drehen sich die Gewichtungen tatsächlich um:\n",
    "\n",
    "$$\\text{df}(\\text{\"bar\"}) = \\frac{1}{4} < \\frac{4}{1} = \\text{idf}(\\text{\"bar\"})$$\n",
    "\n",
    "Oder, weniger mathematisch als Python-Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo    0.693147\n",
       "bla    0.000000\n",
       "baz    1.386294\n",
       "bar    1.386294\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverse_document_frequency(docs: pd.DataFrame):\n",
    "    frequencies = document_frequency(docs)\n",
    "    inverse_frequencies = pd.Series(len(frequencies), index=frequencies.index).div(frequencies)\n",
    "    return inverse_frequencies.apply(np.log)\n",
    "\n",
    "inverse_document_frequency(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wobei wir in unserem Code einen zusätzlichen Schritt eingebaut haben, in dem wir den Logarithmus auf alle Häufigkeiten angewendet haben. Das hat den Grund, dass inverse Häufigkeiten sehr schnell sehr groß werden können (insbes. bei umfangreichen Corpora) und wir diese Entwicklung mittels Logarithmus etwas dämpfen wollen.\n",
    "\n",
    "Außerdem hat es in unserem Fall den schönen Nebeneffekt, dass Begriffe, die in allen Dokumenten vorkommen (also sehr häufig sind), wegen $log(1) = 0$ quasi gecancelt werden :)\n",
    "\n",
    "Und damit, liebe Leser, haben wir den Punkt erreicht, an dem wir nur noch die Teile zusammenzufügen brauchen, um den namensgebenden \"Star\" unseres Artikels zu erhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foo</th>\n",
       "      <th>bla</th>\n",
       "      <th>baz</th>\n",
       "      <th>bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.346574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.396084</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        foo  bla       baz       bar\n",
       "0  0.346574  0.0  0.000000  0.000000\n",
       "1  0.000000  0.0  0.000000  0.346574\n",
       "2  0.099021  0.0  0.396084  0.000000\n",
       "3  0.000000  0.0  0.000000  0.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def term_frequency_inverse_document_frequency(docs: pd.DataFrame) -> pd.DataFrame:\n",
    "    return term_frequency(docs).mul(inverse_document_frequency(docs))\n",
    "\n",
    "term_frequency_inverse_document_frequency(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis dieser Funktion können wir auch direkt verwenden, um die Dokumente in unserem Corpus zu kategorisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foo</th>\n",
       "      <th>bla</th>\n",
       "      <th>baz</th>\n",
       "      <th>bar</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.346574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>foo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346574</td>\n",
       "      <td>bar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.396084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>baz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        foo  bla       baz       bar category\n",
       "0  0.346574  0.0  0.000000  0.000000      foo\n",
       "1  0.000000  0.0  0.000000  0.346574      bar\n",
       "2  0.099021  0.0  0.396084  0.000000      baz\n",
       "3  0.000000  0.0  0.000000  0.000000        -"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorize(docs: pd.DataFrame) -> pd.DataFrame:\n",
    "    tf_idf = term_frequency_inverse_document_frequency(docs).to_dict('index')\n",
    "    for doc, record in tf_idf.items():\n",
    "        max_val = max(record.values())\n",
    "        key = '-'\n",
    "        if max_val > 0.0:\n",
    "            key = [k for k, v in record.items() if v == max_val][0]\n",
    "        tf_idf[doc]['category'] = key\n",
    "    return pd.DataFrame.from_dict(tf_idf, orient='index')\n",
    "\n",
    "categorize(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsere $\\texttt{categorize}()$-Funktion sieht zwar etwas wild aus, das Ergebnis kann sich aber sehen lassen: Für jedes Dokument haben wir den Term mit dem höchsten tf-idf-Score ermittelt und diesen als Kategorienlabel für das Dokument gewählt (bzw. \"-\" falls kein tf-idf-Score größer 0 ist).\n",
    "\n",
    "Übrigens haben wir die Labels der Dokumente streng genommen nicht vorgegeben, sondern unsere Funktion sie auswählen lassen. Tf-idf eignet sich daher auch hervorragend zur Exploration von unbekannten Corpora um zu sehen, in welche \"natürlichen\" Kategorien die Dokumente darin fallen!\n",
    "\n",
    "Das ist, wie gesagt, eine ungemein nützliche Eigenschaft des tf-idf-Verfahrens; in den meisten Fällen ist es aber umgekehrt und die Kategorien meistens grob schon vorgegeben. In dem Fall könnte eine Kategorisierung so aussehen, dass wir pro Kategorie einen festen Satz \"typischer\" Begriffe bereits ermittelt haben und unsere categorize-Funktion dann lediglich anhand des häufigsten Begriffes entscheidet, in welche Kategorie ein Dokument fällt. Eine Kategorie \"foo\" wird vermutlich den Begriff \"foo\" als typischen Begriff deklarieren, weshalb Dokument 1 (bzw. 0) in unserem Corpus in diese Kategorie fiele.\n",
    "\n",
    "Ein anderer Ansatz könnte sein, dass wir einen Teil der Dokumente eines Corpus \"von Hand\" klassifizieren (\"labeln\") und ein ML-Modell darauf trainieren, die Kategorie eines Dokuments zu erkennen. Diese Idee steckt hinter dem namensgebenden \"Sentiment Analysis mit TF-IDF\", der wir uns im zweiten Teil zuwenden wollen:\n",
    "\n",
    "## Teil 2 - Sentiment Analysis mit TF-IDF\n",
    "\n",
    "Sentiment Analysis ist ein Spezialfall von \"document classification\", in dem Dokumente (in unserem Fall Produktreviews von Amazon.com) nur zwei Kategorien \"positiv\" und \"negativ\" zugeordnet werden. Das Vorgehen hier ist wie folgt:\n",
    "\n",
    "1. Amazonreviews laden + ein wenig data cleaning + preprocessing\n",
    "2. Vektorisiere Dokumente aus 1. und berechne tf-idf-Scores\n",
    "3. Verwende Vektoren aus 2. als Input für Classification\n",
    "\n",
    "Bis einschließĺich Schritt 2 ist der Ablauf im Grunde derselbe wie im ersten Teil unseres Artikels. Wo wir vorher aber mittels $\\texttt{categorize()}$ Labels für unsere Dokumente verteilt haben, haben unsere Dokumente schon Labels, d.h. wir trainieren ein Modell, das auf Grundlage der tf-idf-Scores aus Schritt 2 lernen, welche Dokumente eher positiv und welche eher negativ sind.\n",
    "\n",
    "Außerdem wollen wir nicht wieder alles \"from scratch\" implementieren, sondern ungeniert an praxiserprobten Frameworks und Libraries bedienen :)\n",
    "\n",
    "### Schritt 1: Setting the stage\n",
    "\n",
    "Wie oben beschrieben, wollen wir uns im ersten Schritt unsere Product-Reviews laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>rating</th>\n",
       "      <th>Time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  rating        Time  \\\n",
       "0                     1                       1       5  1303862400   \n",
       "1                     0                       0       1  1346976000   \n",
       "2                     1                       1       4  1219017600   \n",
       "3                     3                       3       2  1307923200   \n",
       "4                     0                       0       5  1350777600   \n",
       "\n",
       "                   title                                               text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_reviews = pd.read_csv('data/amazon_misc_products_reviews.csv', nrows=10_000)\n",
    "amazon_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für unsere Zwecke sollen die ersten 10.000 Reviews reichen. Außerdem interessieren uns eigentlich nur die Spalten \"rating\", \"title\" und \"text\" - also weg mit dem Rest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                  title  \\\n",
       "0       5  Good Quality Dog Food   \n",
       "1       1      Not as Advertised   \n",
       "2       4  \"Delight\" says it all   \n",
       "3       2         Cough Medicine   \n",
       "4       5            Great taffy   \n",
       "\n",
       "                                                text  \n",
       "0  I have bought several of the Vitality canned d...  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  This is a confection that has been around a fe...  \n",
       "3  If you are looking for the secret ingredient i...  \n",
       "4  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_reviews = amazon_reviews[['rating', 'title', 'text']]\n",
    "amazon_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das sieht schon besser aus, ist aber noch nicht ganz zufriedenstellend. Zum einen sollen die Dokumente nur als \"positive\" oder \"negative\" gelabelt sein, d.h. wir wollen die Spalte \"rating\" von \\[1 ... 5\\] abbilden auf \"0\" (_negative_) oder \"1\" (_positive_). Und zum Zweiten stellen Titel und Inhalt zusammen ein \"Dokument\" dar, d.h. ich möchte diese beiden Spalten eigentlich gerne in einer einzigen vereinen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    Good Quality Dog Food I have bought several of...\n",
       " 1    Not as Advertised Product arrived labeled as J...\n",
       " 2    \"Delight\" says it all This is a confection tha...\n",
       " 3    Cough Medicine If you are looking for the secr...\n",
       " 4    Great taffy Great taffy at a great price.  The...\n",
       " dtype: object,\n",
       " 0    1\n",
       " 1    0\n",
       " 2    1\n",
       " 3    0\n",
       " 4    1\n",
       " Name: rating, dtype: int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = amazon_reviews[['title', 'text']]\n",
    "reviews = reviews['title'] + \" \" + reviews['text']\n",
    "\n",
    "labels = amazon_reviews[['rating']]\n",
    "labels = labels['rating'].apply(lambda l: 1 if l > 2 else 0)\n",
    "\n",
    "reviews.head(), labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ich habe mich dafür entschieden, dass Reviews mit einem Rating zw. 3 und 5 \"positiv\" sind, ein Rating zw. 1 und 2 dagegen \"negativ\". Darüber kann man sich nun streiten - besonders auch, ob man für Reviews mit Rating 3 nicht eine Kategorie \"neutral\" einführen kann. Alles valide Einwände, für unsere Demonstration aber unerheblich :)\n",
    "\n",
    "Viel wichtiger an dieser Stelle ist der Umstand, dass wir es nicht mehr länger mit einem Mini-Corpus und einem Vokabular aus 4 Fantasiebegriffen zu tun haben, sondern mit tausenden Reviews in natürlicher Sprache (Englisch)!\n",
    "\n",
    "Der letzte Schritt unseres Preprocessings soll also daraus bestehen, dass wir ein bisschen NLP-Magic über unsere Dokumente träufeln. Dazu holen wir uns ein wenig Hilfe von spacy, einem großartigen NLP-Framework für Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def lemmatize(doc: str, lemmatizer):\n",
    "    return ' '.join((t.lemma_.lower() for t in nlp(doc) if t.is_alpha))\n",
    "\n",
    "def preprocess_documents(docs: pd.Series):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "    \n",
    "    return [lemmatize(text, nlp) for text in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das \"heavy lifting\" wird übernommen von $\\texttt{lemmatize()}$, wo, wer hätte es gedacht, ein Text genommen, von allen \"Nicht-Wörtern\" (Zahlen, Satz- und Sonderzeichen etc.) befreit und anschließend jedes Wort auf seine Grundform (linguist. \"Lemma\") reduziert wird.\n",
    "\n",
    "Hintergrund ist der, dass wir davon ausgehen, dass grammatikalische Aspekte (Einzahl / Mehrzahl, Zeitformen, Aspekt etc.) keine Rolle für das Sentiment eines Dokuments spielt und wir deshalb die Menge an Wörtern in unseren Bags of Words merklich reduzieren können. \n",
    "\n",
    "Ein Beispiel dazu: Gramamtikalisch unterscheiden sich die Sätze \"The girl is walking the dogs\" und \"The girl was walking the dog\" in Zeit (present, past) und Numerus (dog - dogs), d.h. unser bag of words würde jeweils einen Eintrag für \"is\", \"was\" und \"dog\" und \"dogs\" enthalten - obwohl diese Unterschiede, wie gesagt, aller Wahrscheinlichkeit nach wenig informativ für das Sentiment eines Dokuments sind.\n",
    "\n",
    "_Lemmatization_ \"entfernt\" diese Aspekte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The girl is walking the dogs -> the girl be walk the dog\n",
      "The girl was walking the dog -> the girl be walk the dog\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The girl is walking the dogs\",\n",
    "    \"The girl was walking the dog\"\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence, \"->\", lemmatize(sentence, nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beide Sätze werden nach dem Preprocessing auf dasselbe Dokument abgebildet - Profit!\n",
    "\n",
    "### Schritt 2: Vectorization und TF-IDF-Scores\n",
    "\n",
    "Und damit sind wir auch schon im zweiten Schritt, der \"Vektorisierung\", angelagt. Theoretisch könnten wir dazu unsere eigene tf-idf-Funktion aus dem ersten Teil verwenden, da der zweite Teil aber unter dem Motto \"das Rad nicht neu erfinden\" steht, holen wir uns wieder ein wenig Hilfe, dieses Mal nicht bei spacy, sondern dem nicht weniger großartigen (und vermutlich sogar wesentlich verbreiteteren) scikit-learn-Framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\texttt{TfidfVectorizer}$ übernimmt hier den gesamten Prozess der Vektorisierung für uns - wir liefern lediglich die (lemmatisierten) Dokumente als Input. Was der Vectorizer auch macht, ist einige zusätzliche Pre- und Postprocessing-Steps, in unserem Fall bspw. \"stopword removal\" (= Entfernen von \"inhaltsleeren\" Begriffen wie Personalpronomen, Konjunktionen etc.), Eindampfen der Bag of Words auf max. 5.000 Begriffe sowie eine gewisse Normalisierung der tf-idf-Scores (L1-Norm, bei Interesse nachzulesen [hier](https://en.wikipedia.org/wiki/Taxicab_geometry)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 300749 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=5_000, norm='l1')\n",
    "\n",
    "lemmatized_documents = preprocess_documents(reviews)\n",
    "features = vectorizer.fit_transform(lemmatized_documents)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir sehen können, besteht unsere \"Dokumentenmatrix\" erwartungsgemäß aus 10.000 Dokumenten (\"Reihen\") mit 5.000 Termen (\"Spalten\"). Diese können wir im nächsten Schritt dazu verwenden, ein Classifikation-Model auf unsere Sentiment Analysis-Task zu trainieren. \n",
    "\n",
    "### Schritt 3: Train and Predict\n",
    "\n",
    "Wieder bedienen wir uns dafür scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für unser Modell habe ich mich für eine Support Vector Machine als Classifier entschieden, aus keinem anderen Grund außer dem, dass ich für gewöhnlich bei ML-Tasks immer erst Support Vector Machines ausprobiere (sofern andere Algorithmen nicht offensichtlich besser geeignet sind).\n",
    "\n",
    "Außerdem habe ich gleich auch Funktionen zum Aufteilen unserer Matrix und Labels in Trainings- und Test-Sets, sowie zum Auswerten der Ergebnisse unserer Klassifizierung.\n",
    "\n",
    "Als Test-Set werden wir 1.000 gelabelte Dokumente zurückhalten und unser Modell auf 9.000 Dokumenten trainieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC()\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(features, labels, test_size=0.1, random_state=0)\n",
    "classifier.fit(xtrain, ytrain)\n",
    "predictions = classifier.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schauen wir uns zuerst direkt an, wie hoch die _accuracy_ unseres Modells auf \"ungesehenen\" Daten ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.899"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytest, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "89,9% - das ist ziemlich gut! Vielleicht zu gut, denn wie bei fast allen Machine Learning-Projekten besteht die Gefahr, dass wir es hier mit Overfitting zu tun haben.\n",
    "\n",
    "Werfen wir deshalb ebenfalls einen Blick auf die _confusion matrix_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative (predicted)</th>\n",
       "      <th>positive (predicted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative (actual)</th>\n",
       "      <td>57</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive (actual)</th>\n",
       "      <td>1</td>\n",
       "      <td>842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   negative (predicted)  positive (predicted)\n",
       "negative (actual)                    57                   100\n",
       "positive (actual)                     1                   842"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(ytest, predictions), index=['negative (actual)', 'positive (actual)'], columns=['negative (predicted)', 'positive (predicted)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die _confusion matrix_ zeichnet ein leicht anderes Bild: Von den 1.000 Testdokumenten waren 843 positiv, die unser Modell auch bis auf eines korrekt als solche erkannt hat. Von den übrigen 157 Dokumenten, die negativ waren, hat unser Modell aber lediglich 57 korrekt als negativ erkannt, 100 negative Dokumente aber als positiv klassifiziert!\n",
    "\n",
    "Das heißt, unser Modell ist zwar gut darin, Reviews mit positivem Sentiment als solche zu erkennen, hat aber beim Erkennen von negativen Reviews deutlichen Nachholbedarf!\n",
    "\n",
    "## Summary\n",
    "\n",
    "Damit sind wir am Ende unseres kleinen Abenteuers mit TF-IDF und Sentiment Analysis angelangt.\n",
    "\n",
    "Im ersten Teil haben wir gelernt, dass TF-IDF ein Verfahren ist, mit dem wichtige Begriffe in einem Dokument erkannt werden können und Dokumente anhand dieser Begriffe dann in Kategorien eingeordnet werden können. Daher das auch geschehen kann, ohne dass im Vorfeld feste Kategorien festgelegt wurden, eignet sich tf-idf auch hervorragend zur Exploration von unbekannten Datensätzen.\n",
    "\n",
    "Sentiment Analysis ist ein Spezialfall von \"Zuordnung von Dokumenten zu Kategorien\", bei dem die Kategorien bereits feststehen (\"positiv\" und \"negativ\") und ein ML-Modell auf vorgelabelten Dokumenten trainiert wird, um später ungelabelte Dokumente diesen beiden Kategorien zuordnen zu können.\n",
    "\n",
    "Diesen Prozess haben wir im zweiten Teil des Artikels beleuchtet, indem wir eine Support Vector Machine als Classifier trainiert haben, das Produktreviews auf Amazon.com als \"positiv\" oder \"negativ\" bewerten kann. Das ging halbwegs gut, wobei das Modell einen deutlichen Bias gegenüber positiven Reviews hatte.\n",
    "\n",
    "Abschließend hoffe ich, dass ich meinen Lesern das Thema \"TF-IDF\" näher bringen konnte. Sollten trotzdem noch Fragen offen sein, Sie das Bedürfnis haben, Lob oder Kritk zum Artikel äußern zu wollen oder allgemein Interesse an NLP haben, dann zögern Sie bitte nicht, entweder auf mich direkt (bevorzugt per Mail!) oder auf die Kollegen bei its-people zuzukommen - wir freuen uns :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
